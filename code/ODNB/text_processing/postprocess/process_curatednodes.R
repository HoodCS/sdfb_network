##@S Examine curated nodes
source("code/ODNB/ODNB_setup.R")

load(zzfile_textproc_post_entitymatrix)
load(zzfile_curated_nodeset_update)

# Helper Functions --------------------------------------------------------
generate_weights = function(rows) {
  ## Input is rows, which are the rows in 'nodeset' that can match one specific mention. 
  
  ## The weights here are used to determine who we think is the more likely owner of a unresolved name mention
  ## higher weights are given to people whose biographies are longer.   
  bio_len = nodeset$ODNB_CORRECT_BIOLENGTH[rows]
  n = length(bio_len)
  basewts = rep((1 / (n + 2)), times = n)
  if (any(is.na(bio_len))) {
    ## weight unknown bios as length 100
    bio_len[is.na(bio_len)] = 100
  }
  return(basewts + ((2 / (n+2)) * bio_len / sum(bio_len)))
}

check_date_overlap = function(docdates, biodates) {
  n = nrow(docdates)
  res = list()
  for(j in 1:n) {
    if (is.na(docdates[j,1])) {
      res[[j]] = 1:nrow(biodates)
    } else {
      res[[j]] = which(biodates[,1] < docdates[j,2] & biodates[,2] > docdates[j,1])
    }
  }
  return(res)
}

## TODO: [Move code] Move and document this function...? but not generally usable. 
## Can check for exact dates ('IN' for both later. don't need this function...)
extract_searchable_date = function(nodeset_row) {
  # for birth date: 
  b = nodeset_row$ext_birth
  birth = switch(gsub("/", "", nodeset_row$AF.BF.CA.IN), AF = b, AFIN = b - 1, BF = b - 20, BFIN = b - 20, CA = b - 20, IN = b - 1)
  d = nodeset_row$ext_death
  death = switch(gsub("/", "", nodeset_row$AF.BF.CA.IN2), AF = d + 20, AFIN = d + 20, BF = d, BFIN = d + 1, CA = d + 20, IN = d + 1)
  if (length(c(birth, death)) < 2) {
    if (birth > 0) {
      death = birth + 50
    } else {
      death = birth - 50 
    }
  }
  return(c(birth, death))
}

convert_entitymatrix_into_format = function(em, correct_ids) {
  ## dropped params  docn = NULL, entity = NULL, 
  ## either pass in true docnums or entity vector, and corresponding correct_ids, 
  docuseg = as.character(em$DocumentNum + em$Segment / 100)
#   if (!is.null(docn)) {
#     ID = correct_ids[match(em$DocumentNum, docn)]
#   } else if (!is.null(entity)) {
#     ID = correct_ids[match(em$Entity, entity)]
#   } else {
    ## if neither docn, entity passed in: then correct_ids is vector of appropriate SDFB_IDs. 
    ## in this case, drop rows with NAs. 
    ID = correct_ids
    res = data.frame(SDFB_ID = ID, DocNum = docuseg, Count = em$Count)
    res = res[!(is.na(ID) | ID == 0),]
    return(res)
#   }
#   res = data.frame(SDFB_ID = ID, DocNum = docuseg, Count = em$Count)
#   return(res)
}


Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}


# Code --------------------------------------------------------------------


# Create a list of named entities to check for ----------------------------
## Now, we are given a curated nodeset (and each possibly with alternate names -- the work of disambiguation)
##   This step combines all of this into one list -- multiple nodes may request searching the same exact name
##   This step produces a list where each name is identified with a number of nodes that correspond to it. 

## Examine accents in search_all
accents = which(gsub("[], --\\.'[:alpha:]]", "", nodeset$search_names_ALL) != "")

library(stringr)
## For names with accents, use firstname/surname pair as additional name to search. 
search_names = lapply(strsplit(nodeset$search_names_ALL, ","), function(x) {gsub(" +", " ", str_trim(x))})
firstlast_pair = paste(nodeset$first_name, nodeset$surname, sep = " ")

search_vector = unique(c(c(search_names, recursive = TRUE), firstlast_pair))
search_idlist = list()
## now, check the id list against the search_names fields inside nodeset
# might not be the most efficient direction; runs one time so whatever... 
for(j in seq_along(search_vector)) {
  search_idlist[[j]] = sort(unique(c(which(sapply(search_names, function(x) {any(x == search_vector[j])})),
                                     which(search_vector[[j]] == firstlast_pair)
  )))
  if (j %% 25 == 0) { print(j) }
}
rm(accents, search_names, j)

## now, the pair search_vector / search_idlist provides information -- 
# search_vector is a vector of character strings to search
# search_idlist provides the corresponding node indices that match each character string (each actual name)



## For each node in the provided nodeset, assign the corresponding ODNB article as their biography if it makes sense to do so (there is a very high-count named entity that is exactly the node name (or very close to the node name -- spelling issues?))
## Store the old ID numbers (so that stuff can be referenced backwards)
fix_entitymatrix = data.frame(big_entity_matrix, OLD_ID = big_entity_matrix$ID)

for(j in seq_along(nodeset$ODNB_CORRECT_ID)) {
  ## IF is ODNB article
  if (is.na(nodeset$ODNB_CORRECT_ID[j])) {
    ## do nothing -- this node does not have an ODNB article
  } else {
    id = nodeset$ODNB_CORRECT_ID[j]
    rows = which(fix_entitymatrix$DocumentNum == id)
    ## If no main-author of the article, this might need to be fixed -- 
    if (!(any(fix_entitymatrix$ID[rows] == 1))) {
      ## If there is a close match in top 5 of count: 
      basenames = names(sort(tapply(fix_entitymatrix$Count[rows], as.character(fix_entitymatrix$Entity[rows]), sum), decreasing = TRUE))
      basenames = basenames[1:min(5, length(basenames))]
      matches = agrep(pattern = firstlast_pair[j], x = basenames, max.distance = 0.4)
      if (length(matches) > 0) {
        fix_entitymatrix$ID[rows[which(fix_entitymatrix$Entity[rows] == basenames[matches[1]])]] = 1
      }
    }
  }
  if (j %% 25 == 0) { print(j) }
}
rm(firstlast_pair, basenames, matches, id, rows, j)



# Ran until here ------ ---------------------------------------------------




## Three types of matches
## bio matches => ID = 1
## unique matches => ID != 1
## shared matches
docids = nodeset$ODNB_CORRECT_ID[!is.na(nodeset$ODNB_CORRECT_ID)]
bio_matches = fix_entitymatrix[fix_entitymatrix$ID == 1, ]
bio_matches = bio_matches[bio_matches$DocumentNum %in% docids,]
doc_match = match(bio_matches$DocumentNum, nodeset$ODNB_CORRECT_ID)

# Sanity check. 
test = convert_entitymatrix_into_format(bio_matches,correct_ids = doc_match)
cbind(nodeset$ODNB_CORRECT_ID[test$SDFB_ID], test)
# convert_entitymatrix_into_format(bio_matches, docn = docids, correct_ids = seq_along(docids))

sub_entitymatrix = fix_entitymatrix[fix_entitymatrix$ID > 1,]
fuzziness = 10 # years fuzzy on bio date extraction
all_matches = match(sub_entitymatrix$Entity, search_vector)

# Store results
exact_match = rep(NA, times = length(all_matches))
partial_list = list()
partcount = 0

for(j in seq_along(search_vector)[-1]) { #1 is NA
  print(j)
  matches = which(all_matches == j)
  if (length(matches) > 0) {
    docdates = cbind(sub_entitymatrix$MinDate[matches] - fuzziness, sub_entitymatrix$MaxDate[matches] + 10)
    biodates = lapply(search_idlist[[j]], function(x) {extract_searchable_date(nodeset[x,])})
    biodates = do.call(rbind, biodates)
    rownames(docdates) = NULL
    dateoverlap = check_date_overlap(docdates, biodates)
    for(k in seq_along(dateoverlap)) {
      subid = matches[k]
      if (length(dateoverlap[[k]]) > 0) {
        if (length(dateoverlap[[k]]) == 1) {
          exact_match[subid] = search_idlist[[j]][dateoverlap[[k]]]
        } else {
          partcount = partcount+1
          docuseg = as.character(sub_entitymatrix$DocumentNum[subid] + sub_entitymatrix$Segment[subid] / 100)
          partial_list[[partcount]] = list(Doc = docuseg, Count = sub_entitymatrix$Count[subid],
                                           IDs = search_idlist[[j]][dateoverlap[[k]]], 
                                           wts = generate_weights(search_idlist[[j]][dateoverlap[[k]]]))
        }
      }
    }
  }
}

non_bio_exact_match = convert_entitymatrix_into_format(em = sub_entitymatrix, correct_ids = exact_match)
# partial_list

exact_df = rbind(convert_entitymatrix_into_format(bio_matches,correct_ids = doc_match), non_bio_exact_match)
# head(partial_list)

out_df = lapply(partial_list, 
                function(x) {res = data.frame(SDFB_ID = x$IDs, DocNum = x$Doc, Count = x$Count, Weight = x$wts)
                             return(res)})
partial_df = do.call(rbind, out_df)

## TODO: [DOCUMENT] this data format
#save(exact_df, partial_list, partial_df, file = zzfile_base_entity_matrix)
#save(search_idlist, search_vector, fix_entitymatrix, file = zzfile_base_entity_matrix_FULLDATA)



